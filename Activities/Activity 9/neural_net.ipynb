{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from keras.datasets import mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform one-hot encoding\n",
    "train_Y = np.zeros((train_y.max()+1, train_y.shape[0])) \n",
    "train_Y[train_y, np.arange(train_y.shape[0])] = 1\n",
    "test_Y = np.zeros((test_y.max()+1, len(test_y)))\n",
    "test_Y[test_y, np.arange(len(test_y))]=1 \n",
    "np.shape(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, max_iters=100, eta=0.5):\n",
    "        self.max_iters = max_iters \n",
    "        self.eta = eta \n",
    "\n",
    "    def set_trainData(self, X_train, y_train): \n",
    "        self.X_ = X_train \n",
    "        d = np.copy(y_train)\n",
    "        d[d<=0] = 0 \n",
    "        self.y_ = d\n",
    "        return self \n",
    "    \n",
    "    def fit(self):\n",
    "        self.w_ = np.random.rand(1 + self.X_.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            error = 0\n",
    "            delta_wj = np.copy(self.w_)\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            indices = np.arange(len(self.X_))\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = self.X_[indices]\n",
    "            y_shuffled = self.y_[indices]\n",
    "            \n",
    "            for xj, di in zip(X_shuffled, y_shuffled):\n",
    "                update = self.eta * (di - self.predict(xj))\n",
    "                self.w_[0] += update \n",
    "                self.w_[1:] += update * xj \n",
    "                error += np.abs(update / self.eta)\n",
    "            self.errors_.append(error)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def weights(self):\n",
    "        return self.w_ \n",
    "    \n",
    "    def errors(self):\n",
    "        return self.errors_ \n",
    "    \n",
    "    def predict(self, X):\n",
    "        a=np.dot(X, self.w_[1:]) + self.w_[0] \n",
    "        return np.where(a >= 0, 1, 0) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network: \n",
    "\n",
    "    def __init__(self, layers=5, neuron=5):\n",
    "        self.layers = layers \n",
    "        self.neuron = neuron \n",
    "\n",
    "    def set_train(self, train_data, targets):\n",
    "        self.x_train, self.y_train = train_data[0], train_data[1]\n",
    "        self.train_data =  [(self.x_train[i,:,:], self.y_train[:,i]) for i in range(np.shape(self.x_train)[0])] \n",
    "\n",
    "        # Initialize random weights. Returns a list containing: \n",
    "        # Mx(neuron) 2D array in the first index, where M is the number of initial neurons, and (neuron)xN, where N is the number of neurons in the output layer, and (neuron)x(neuron) in the middle layers.\n",
    "        self.w_ = [np.random.rand(self.neuron, self.neuron) for i in range(self.layers-1)]\n",
    "        input_w, output_w = np.random.rand(self.neuron, len(self.x_train[0].flatten())), np.random.rand(targets, self.neuron)\n",
    "        self.w_.insert(0, input_w)\n",
    "        self.w_.append(output_w)\n",
    "\n",
    "        # Initialize random column bias vectors. Returns a list containing: \n",
    "        # 1xM row vector in the first index, where M is the number of initial neurons, and 1xN, where N is the number of neurons in the output layer, and 1x(neuron) in the middle layers.\n",
    "        self.bias_ = [np.random.rand(self.neuron, 1) for i in range(self.layers)]\n",
    "        input_b, output_b = np.zeros((len(self.x_train[0].flatten()),1) ), np.random.rand(targets,1)\n",
    "        self.bias_.insert(0, input_b)\n",
    "        self.bias_.append(output_b)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def feedforward(self, input):\n",
    "        input = np.column_stack([np.array(input)])\n",
    "        initial_act=  input + np.array(self.bias_[0])\n",
    "        self.activations_ = [initial_act] \n",
    "        self.z_ = [np.array(input)]\n",
    "        for l in range(0,self.layers+1): #Loop over l = 0,1,2,...,L-1 where L-1 is the output layer \n",
    "            z = np.matmul(self.w_[l], np.column_stack([np.array(self.activations_[l])])) + self.bias_[l+1]\n",
    "            self.z_.append(z)\n",
    "            self.activations_.append(self.threshold(z))\n",
    "        return self\n",
    "    \n",
    "    def output_error(self, output):\n",
    "        err = (self.activations_[-1] - np.column_stack([output]))*self.threshold_der(self.z_[-1])\n",
    "        return err \n",
    "    \n",
    "    def cost(self, output):\n",
    "        return (0.5)*np.linalg.norm(self.activations_[-1] - output)**2\n",
    "    \n",
    "    def backpropagate(self, output):\n",
    "        self.error_array = [self.output_error(output)]\n",
    "        for l in range(-1, -self.layers-1, -1):\n",
    "            error_vector = np.dot(np.transpose(np.array(self.w_[l])), np.array(self.error_array[l]))*np.column_stack([np.array(self.threshold_der(self.z_[l-1]))])\n",
    "            self.error_array.insert(0, error_vector)\n",
    "        return self\n",
    "    \n",
    "    def threshold(self, z): #Hyperbolic tangent\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def threshold_der(self, z): #Derivative of the activation function \n",
    "        return 1 + self.threshold(z)**2\n",
    "\n",
    "    \n",
    "    def train(self, test, epochs = 10, eta = 0.01, batch_size=100, ): \n",
    "        for iter in range(epochs+1): \n",
    "            if test != None:\n",
    "                print('Epoch {a}/{b}: {c}'.format(a=iter, b= epochs, c=self.evaluate(test)))\n",
    "            np.random.shuffle(self.train_data) \n",
    "            mini_batches = [self.train_data[k:k+batch_size] for k in range(0,len(self.train_data), batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                for (m,n) in mini_batch:\n",
    "                    self.feedforward(np.array(m.flatten()))\n",
    "                    self.backpropagate(n) \n",
    "                    for l in range(-1, -self.layers-2, -1): \n",
    "                        update_w = np.dot(self.error_array[l], self.activations_[l-1].T)\n",
    "                        update_b = self.error_array[l] \n",
    "                        self.w_[l] -= update_w*(eta/batch_size)\n",
    "                        self.bias_[l] -= update_b*(eta/batch_size)\n",
    "        return self.w_, self.bias_\n",
    "    \n",
    "    def evaluate(self, test_data): \n",
    "        i = 0\n",
    "        for (input,output) in test_data:\n",
    "            input = np.array(input).flatten()\n",
    "            input = np.column_stack([np.array(input)])\n",
    "            a_list = [input] \n",
    "            z_list = [input]\n",
    "            for l in range(0,self.layers+1):\n",
    "                z = np.matmul(self.w_[l], np.column_stack([np.array(a_list[l])])) + self.bias_[l+1]\n",
    "                z_list.append(z)\n",
    "                a_list.append(self.threshold(z))\n",
    "            if np.argmax(a_list[-1]) == output:\n",
    "                i += 1\n",
    "        return i/len(test_data)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network1(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print (\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20: 0.098\n",
      "Epoch 1/20: 0.8314\n",
      "Epoch 2/20: 0.8467\n",
      "Epoch 3/20: 0.8446\n",
      "Epoch 4/20: 0.8421\n",
      "Epoch 5/20: 0.8425\n",
      "Epoch 6/20: 0.8381\n",
      "Epoch 7/20: 0.8405\n",
      "Epoch 8/20: 0.8542\n",
      "Epoch 9/20: 0.838\n",
      "Epoch 10/20: 0.8372\n",
      "Epoch 11/20: 0.8359\n",
      "Epoch 12/20: 0.8406\n",
      "Epoch 13/20: 0.8411\n",
      "Epoch 14/20: 0.844\n",
      "Epoch 15/20: 0.8444\n",
      "Epoch 16/20: 0.8446\n",
      "Epoch 17/20: 0.8457\n",
      "Epoch 18/20: 0.8275\n",
      "Epoch 19/20: 0.8253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[0.28932597, 0.01276585, 0.40338334, ..., 0.08143744, 0.24012045,\n",
       "          0.26617638],\n",
       "         [0.20155605, 0.81251396, 0.00322621, ..., 0.96390381, 0.82542192,\n",
       "          0.60490117],\n",
       "         [0.75238705, 0.72394835, 0.01593775, ..., 0.49553607, 0.76419978,\n",
       "          0.15996469],\n",
       "         ...,\n",
       "         [0.3653103 , 0.41014683, 0.88756638, ..., 0.76303681, 0.2514801 ,\n",
       "          0.71449704],\n",
       "         [0.31408989, 0.2042067 , 0.21751345, ..., 0.6170919 , 0.98864954,\n",
       "          0.40543242],\n",
       "         [0.45559055, 0.47320946, 0.33637646, ..., 0.16282167, 0.19293314,\n",
       "          0.71000183]]),\n",
       "  array([[-6.61487050e-03,  7.32242711e-03,  1.47918500e-02,\n",
       "           8.68945314e-03, -5.95528803e-03, -1.18966806e-02,\n",
       "          -6.98800391e-03, -1.30561464e-02,  5.97854378e-03,\n",
       "          -2.99094562e-02, -5.23451157e-03,  3.88600554e-03,\n",
       "          -2.12643199e-02, -1.14814053e-02, -1.93727294e-02,\n",
       "           1.01775157e-02, -3.42635953e-03,  8.47934333e-03,\n",
       "           6.22311328e-03,  3.60933456e-03,  4.84496295e-02,\n",
       "           1.99513997e-02,  2.45417518e-03, -6.58287258e-02,\n",
       "          -9.58502580e-04,  4.15134666e-03, -1.05854166e-02,\n",
       "          -1.55143039e-02,  9.53586557e-03, -4.56535844e-02,\n",
       "          -1.50793361e-02, -4.14399609e-03, -6.57025028e-03,\n",
       "           1.02918039e-02, -4.20695081e-03,  1.48451241e-02,\n",
       "          -3.57618740e-03, -8.14925497e-03,  5.27569358e-03,\n",
       "          -3.91055009e-03, -2.17894569e-02, -5.44500791e-03,\n",
       "          -1.53576834e-02, -3.01311240e-03, -8.78365898e-03,\n",
       "          -8.60433940e-03, -1.23881339e-02, -5.94337029e-02,\n",
       "           1.13614154e-02, -3.19844701e-02, -1.64914787e-03,\n",
       "          -5.25243684e-03, -2.81762560e-02, -2.86828603e-02,\n",
       "          -3.07240315e-03, -2.92841647e-03, -8.85503677e-03,\n",
       "           4.39173416e-02,  2.65883141e-03, -6.88069398e-03,\n",
       "           1.17434343e-03, -1.34828818e-02,  1.24837478e-02,\n",
       "          -2.02382214e-03, -4.66817140e-03,  3.23445235e-03,\n",
       "           5.02248845e-03, -1.49767432e-02,  6.42823379e-03,\n",
       "          -5.90998486e-02, -7.19027061e-03, -1.60911776e-03,\n",
       "          -1.09488153e-02,  2.62237218e-03,  1.48853512e-02,\n",
       "           5.05372091e-03,  1.47420108e-02,  1.08618856e-02,\n",
       "          -3.93668004e-03,  2.14885233e-02, -1.25547193e-02,\n",
       "          -1.19148038e-02,  1.59852410e-02,  2.10836919e-03,\n",
       "          -9.26272137e-04, -1.80691566e-02, -7.88562651e-03,\n",
       "           8.18463694e-03, -1.13395837e-02,  6.53912621e-03,\n",
       "          -1.67596723e-02, -1.86176501e-02, -1.17914277e-02,\n",
       "           3.28431967e-03,  2.24655892e-02,  1.61414187e-02,\n",
       "          -6.38718169e-03, -4.27848868e-03,  1.19648877e-02,\n",
       "           7.90392117e-02],\n",
       "         [ 9.34030688e-03, -2.21277156e-02,  2.16043519e-02,\n",
       "          -2.34672722e-02, -2.50091333e-02, -2.74887246e-03,\n",
       "           1.65055197e-02,  1.04873653e-01,  1.88333924e-02,\n",
       "           1.88985888e-02,  1.58919679e-02,  7.51655679e-02,\n",
       "          -1.76221952e-02,  7.91931628e-03,  4.92546423e-03,\n",
       "           1.32369431e-02,  4.77222201e-03,  2.20025924e-03,\n",
       "           3.61414713e-02, -1.32490039e-03, -2.55288654e-02,\n",
       "          -1.93300699e-02,  7.98072134e-03, -8.48756841e-03,\n",
       "           4.31032325e-02,  2.29165185e-02,  3.69894713e-02,\n",
       "          -1.24806728e-02, -7.22534337e-02,  7.62848135e-02,\n",
       "          -1.95761904e-03, -3.59431554e-02, -5.88956377e-02,\n",
       "          -3.43216513e-02, -1.47255842e-01,  1.40908396e-03,\n",
       "          -1.62054118e-02, -2.19659843e-02, -1.53172274e-01,\n",
       "           1.82078808e-02, -2.84520239e-02, -1.17557971e-01,\n",
       "          -2.61179978e-05, -2.32156977e-02,  6.63846405e-03,\n",
       "           7.01577970e-03,  9.05211824e-02, -5.70561198e-02,\n",
       "           2.69098205e-02,  2.65185464e-03, -3.62765255e-02,\n",
       "           1.69593485e-02, -1.39497814e-02, -9.23714292e-03,\n",
       "          -2.84626358e-03, -5.67230327e-03,  1.10775086e-02,\n",
       "           5.81900344e-03, -6.35553244e-02, -1.33778959e-02,\n",
       "           2.94123021e-03, -9.92842535e-03, -1.74101516e-02,\n",
       "          -1.06991331e-02,  1.41638750e-02, -7.28793912e-03,\n",
       "           1.08038549e-01,  1.88457934e-02, -3.17938979e-02,\n",
       "           1.72384142e-02,  8.44502949e-02,  4.56558078e-02,\n",
       "          -1.49372538e-03, -1.33685326e-01, -3.36968538e-02,\n",
       "           1.60727258e-02,  2.38848103e-02, -4.67495909e-02,\n",
       "          -3.80420501e-02,  2.96258702e-02, -3.42829979e-03,\n",
       "           1.61372848e-02, -2.47101819e-02, -1.74629784e-02,\n",
       "           2.89956145e-03,  9.97780300e-03, -1.45028229e-02,\n",
       "           2.45633311e-02,  1.19046478e-02, -2.28461905e-02,\n",
       "          -1.27893723e-02,  1.89971775e-02,  2.95451668e-03,\n",
       "           2.64019680e-02, -1.91279153e-02, -4.56337220e-02,\n",
       "          -1.74651212e-02, -4.58768537e-03, -2.50723499e-02,\n",
       "           4.53897481e-03],\n",
       "         [ 2.06827195e-02,  1.60834491e-03, -7.69694142e-03,\n",
       "           3.23321363e-02,  2.55291216e-02,  2.33651186e-02,\n",
       "           5.76022141e-03, -1.19056273e-02, -7.84991321e-03,\n",
       "           2.48370779e-02, -5.95929065e-03, -7.71406113e-03,\n",
       "           4.89220704e-02, -1.07287049e-02, -2.27253200e-02,\n",
       "          -2.59376959e-02, -2.55419180e-03, -7.67332713e-03,\n",
       "           1.85446958e-03, -1.75354557e-03, -7.87286007e-03,\n",
       "           1.29613815e-03,  4.41208207e-03,  3.37800626e-03,\n",
       "          -5.62651067e-03,  4.10266765e-03, -1.28430924e-02,\n",
       "          -4.57602646e-03,  3.64179703e-02, -6.81856556e-03,\n",
       "          -9.02516274e-04,  7.26143664e-03, -3.19517758e-03,\n",
       "           4.75074637e-03,  5.47232221e-03,  1.59474607e-03,\n",
       "          -1.81420775e-03, -9.81261839e-03,  9.49034299e-03,\n",
       "          -1.12980127e-02,  1.43315146e-02, -1.44748409e-02,\n",
       "           3.31962493e-02,  2.31824469e-02, -4.61109933e-03,\n",
       "           2.65919173e-03, -2.01361809e-02, -7.74301736e-02,\n",
       "          -9.84058120e-03,  2.63101411e-02, -7.61962465e-03,\n",
       "          -1.42298978e-02, -2.86759428e-02,  1.20935692e-02,\n",
       "           1.28125688e-03, -1.82798997e-02,  2.00635705e-02,\n",
       "          -3.25315487e-04,  9.32945948e-03,  6.02132281e-03,\n",
       "           6.30957738e-03,  3.22354462e-03,  1.84316029e-02,\n",
       "          -6.79993228e-03,  1.57678300e-02,  2.19969322e-02,\n",
       "           3.87394155e-03,  5.07559843e-03,  1.59374883e-02,\n",
       "           1.83483952e-02, -2.69590284e-03, -1.90359956e-02,\n",
       "           3.00555321e-03,  2.53564574e-02,  1.96331332e-02,\n",
       "          -1.95801589e-03, -1.91926788e-02, -4.41519426e-03,\n",
       "           9.20764878e-03, -1.91016794e-02,  1.51113913e-02,\n",
       "           3.35544013e-03, -2.08463968e-02, -2.36732552e-02,\n",
       "           1.20195399e-02,  1.71330056e-02,  6.49069632e-02,\n",
       "           2.05005206e-02, -2.79866317e-04, -3.33672584e-02,\n",
       "           2.62083053e-02,  1.47672683e-02,  5.26531440e-03,\n",
       "           9.24401235e-03, -9.20543688e-03,  1.78907960e-02,\n",
       "          -1.78424305e-02, -1.15645817e-02,  2.81104757e-03,\n",
       "          -1.03453621e-02],\n",
       "         [-2.52315849e-02, -9.88856660e-03, -8.72991953e-03,\n",
       "          -1.31067985e-02, -7.63322337e-03, -2.20302527e-02,\n",
       "          -2.88056468e-02, -1.00655291e-02,  1.36697448e-02,\n",
       "          -2.00761940e-02, -8.52205158e-03,  5.19947010e-03,\n",
       "          -8.01177124e-04, -9.08001008e-03,  8.03564442e-04,\n",
       "           6.98765488e-03, -1.90234859e-03, -5.07266446e-03,\n",
       "          -4.72589196e-02,  7.32390511e-03,  7.56394554e-04,\n",
       "           7.89237017e-03,  7.95634005e-03,  7.70708263e-03,\n",
       "          -5.29218218e-03, -3.51418796e-02,  7.14476367e-03,\n",
       "           2.91638668e-02,  5.70375552e-04,  5.07980594e-02,\n",
       "           8.42361336e-03,  5.68703563e-03,  3.43692192e-03,\n",
       "          -6.04133390e-02, -4.87426358e-03, -2.05541460e-02,\n",
       "          -1.58539863e-02, -6.97778098e-03, -1.65235763e-02,\n",
       "          -1.03830303e-02,  2.26088044e-02, -1.70453582e-02,\n",
       "          -2.36264558e-03,  1.56206193e-02,  3.32163130e-03,\n",
       "          -2.07894840e-02, -3.01450664e-02, -5.14919408e-02,\n",
       "          -2.29326065e-02,  1.02169187e-02, -9.80668140e-03,\n",
       "           1.90515973e-02, -1.40723534e-02,  5.01331789e-03,\n",
       "           2.51597623e-03,  3.68350179e-02,  3.35968143e-02,\n",
       "           1.35535238e-02, -8.77115900e-03,  4.52306030e-03,\n",
       "           1.76888580e-03,  1.54603773e-02, -5.80699279e-03,\n",
       "           2.10581932e-02, -2.63836962e-03, -2.50057840e-02,\n",
       "          -9.02413313e-04,  5.42906283e-03,  1.94211356e-02,\n",
       "          -6.09910302e-02, -1.35979483e-03, -1.51520668e-02,\n",
       "           1.67818656e-02, -1.20836761e-02,  8.70036341e-03,\n",
       "           1.32238512e-02, -2.52219267e-02,  1.01298540e-03,\n",
       "           4.26276321e-03, -1.17835528e-02, -9.49542133e-03,\n",
       "          -2.78296112e-04,  1.72443943e-03,  1.25844324e-02,\n",
       "          -9.08485916e-03, -5.25203603e-03, -1.32560096e-02,\n",
       "          -9.27230212e-04,  1.43574541e-03,  1.88222868e-02,\n",
       "          -3.70511048e-03,  1.76538108e-02, -6.86275306e-03,\n",
       "          -1.04293400e-02, -1.40325330e-03, -3.25634191e-03,\n",
       "           2.17715443e-02,  4.63648577e-04, -3.80066357e-03,\n",
       "          -8.00048835e-04],\n",
       "         [ 1.71717314e-02, -7.80236557e-03, -2.44594552e-02,\n",
       "          -1.04571924e-02, -1.08836094e-02, -1.44045996e-01,\n",
       "           2.22671456e-02,  2.37037934e-02, -6.10439849e-02,\n",
       "          -2.83607666e-02,  6.53912553e-03,  1.53707473e-02,\n",
       "           4.35590861e-02,  2.51907279e-02, -4.22493943e-03,\n",
       "          -4.71971087e-03,  1.86697361e-02, -3.72274171e-03,\n",
       "           1.88267469e-02, -1.03958317e-02, -4.19007071e-03,\n",
       "           5.82605712e-02,  1.02766701e-02, -9.67219452e-04,\n",
       "           5.82328382e-03,  2.62804614e-03, -2.09105603e-02,\n",
       "           1.20635886e-02,  5.33469989e-02,  1.45308073e-01,\n",
       "           5.45011853e-03,  2.13607549e-03,  2.50823990e-03,\n",
       "           2.34698540e-02, -2.51660876e-03, -4.29973485e-04,\n",
       "          -2.22681452e-02, -1.61036528e-02,  3.98387994e-03,\n",
       "          -2.44612490e-02, -1.62834272e-02,  1.70120210e-02,\n",
       "           1.88020178e-01, -2.79612880e-03, -4.03021605e-03,\n",
       "           8.05492772e-04,  3.37193364e-02, -3.96367373e-03,\n",
       "          -9.11940251e-03,  3.25408876e-02, -1.43291832e-02,\n",
       "           1.21422653e-02,  3.24626350e-03,  1.31161705e-02,\n",
       "          -1.71810227e-03, -1.31876921e-02, -4.62629751e-03,\n",
       "           1.33593806e-02, -1.69121945e-02,  1.25611990e-02,\n",
       "          -2.14113582e-02,  4.62814826e-03,  9.48807881e-03,\n",
       "           2.70706750e-02, -8.46159856e-03, -1.06840506e-02,\n",
       "          -6.38511141e-04,  1.68849512e-02,  2.59768455e-02,\n",
       "           1.94602099e-02,  1.55094251e-02,  1.01782902e-03,\n",
       "          -1.64740358e-03,  7.95442705e-03, -1.17941303e-02,\n",
       "           1.63829573e-02, -3.59904302e-03,  3.36465513e-03,\n",
       "           2.97720744e-02,  1.60296391e-02,  1.32438183e-03,\n",
       "           1.23220953e-02, -2.12187416e-02,  6.38326679e-03,\n",
       "           3.03266557e-02, -1.72454181e-02, -7.32192746e-03,\n",
       "          -4.87222204e-02, -1.85247708e-04, -3.57679798e-03,\n",
       "           2.19795825e-02,  1.64280717e-02,  1.29195222e-03,\n",
       "          -1.22404603e-03, -1.21100481e-02,  6.50700792e-03,\n",
       "           9.07416497e-02,  7.48257283e-03, -5.06716240e-02,\n",
       "           1.35543330e-02],\n",
       "         [-1.13551853e-02,  1.64269140e-02, -1.00584763e-02,\n",
       "           8.17751027e-04,  5.90852263e-02,  1.01590968e-02,\n",
       "          -2.85579533e-03, -1.36221124e-03,  1.97519344e-02,\n",
       "           7.05242952e-03,  5.51754743e-03,  3.44182658e-05,\n",
       "          -5.46618548e-03, -5.07758763e-03, -7.04919651e-03,\n",
       "          -6.06179699e-03, -1.63879024e-02,  3.00343705e-03,\n",
       "          -1.24297707e-02,  3.75655432e-02, -3.77588162e-02,\n",
       "          -3.00620506e-02, -4.23217097e-03,  5.69467086e-03,\n",
       "           3.01865544e-02,  1.03744318e-02, -4.43011181e-02,\n",
       "           5.01134738e-03,  3.74890426e-02, -2.12097291e-01,\n",
       "           3.66098746e-03,  6.95590008e-03,  2.11515643e-03,\n",
       "          -2.00871754e-02,  2.33368686e-02,  2.11867084e-02,\n",
       "          -3.12468425e-03,  2.01738606e-02,  4.68022355e-03,\n",
       "          -1.58829615e-02,  3.95102745e-02,  1.94909076e-03,\n",
       "           9.57090944e-03,  3.42297700e-03, -3.33727215e-03,\n",
       "          -4.62652980e-03,  4.92027211e-03,  7.82120792e-02,\n",
       "           4.67029781e-03, -1.82160781e-02,  2.88985430e-02,\n",
       "          -3.05427264e-02, -1.01786473e-02, -2.31356977e-02,\n",
       "           1.03295100e-02, -4.73848546e-02, -3.02936911e-02,\n",
       "          -4.84274151e-02,  1.45592871e-02,  8.56913144e-04,\n",
       "          -1.62831693e-02, -1.59609218e-02, -2.24077559e-02,\n",
       "          -1.80864895e-02, -4.54873085e-03, -2.54325714e-02,\n",
       "           2.78466915e-03, -5.02056806e-03, -8.45707233e-03,\n",
       "          -1.79731375e-02,  3.27260032e-03,  2.28580154e-03,\n",
       "           1.64256856e-02,  6.89328484e-03,  3.16687193e-02,\n",
       "          -3.35914202e-02,  4.69008928e-03, -2.14400916e-02,\n",
       "          -4.74576921e-02, -3.98091090e-02, -1.53967606e-02,\n",
       "          -1.22812129e-02,  2.27505492e-02, -3.04480443e-02,\n",
       "           4.92172872e-04,  2.09401537e-02,  9.57126199e-03,\n",
       "           3.84120439e-04,  2.48545852e-03,  1.39861698e-02,\n",
       "           1.20718690e-02,  1.52552406e-02,  1.53522787e-02,\n",
       "           4.55753335e-03, -9.33813261e-03,  7.89548568e-03,\n",
       "          -4.17706559e-02,  1.31506086e-02,  2.51501420e-02,\n",
       "          -4.86505145e-02],\n",
       "         [-1.02472980e-02, -9.93534254e-03,  1.00811685e-02,\n",
       "           1.71652788e-03, -2.10733002e-02, -6.93066292e-03,\n",
       "          -7.31995960e-03, -6.72406399e-03,  3.34401760e-02,\n",
       "           5.79216546e-02, -4.45923864e-03, -1.13542004e-02,\n",
       "           3.16475265e-02,  9.76505029e-04,  1.11722443e-02,\n",
       "           3.31817683e-02, -2.27671119e-02,  9.90640004e-03,\n",
       "          -2.71151300e-02, -8.34344424e-03,  1.86536770e-02,\n",
       "           2.34817664e-02,  2.44265213e-03,  2.18576757e-02,\n",
       "          -2.80890682e-02, -6.51315964e-03,  1.63927785e-03,\n",
       "          -1.06790018e-02, -1.39067423e-01, -3.39372534e-02,\n",
       "          -1.47294776e-02,  1.29951730e-03,  2.64616222e-03,\n",
       "           6.67634310e-03, -6.28765313e-03, -1.59547981e-02,\n",
       "           5.83471255e-03,  1.28582235e-02, -1.19775598e-02,\n",
       "           1.67073983e-02, -7.52500619e-03,  6.68531036e-02,\n",
       "          -1.59807425e-01,  5.18328912e-04, -1.39929017e-02,\n",
       "           3.87896342e-03, -1.01462719e-02,  4.96286076e-02,\n",
       "          -1.18109304e-02, -8.22150825e-02, -7.46275610e-04,\n",
       "           1.33532627e-02, -2.62253582e-02, -1.13502740e-03,\n",
       "          -7.54011525e-03,  1.46184034e-02,  5.67999145e-03,\n",
       "          -9.55113956e-03, -5.44973514e-03, -3.59695194e-03,\n",
       "           1.55170294e-02,  1.54291539e-02, -4.23563235e-03,\n",
       "          -1.93645899e-02,  1.12465512e-02,  4.55047568e-03,\n",
       "          -4.17134849e-04, -1.34089114e-02,  2.90597020e-03,\n",
       "           1.75013058e-02,  1.00463179e-02, -1.28659499e-02,\n",
       "           2.56845358e-04, -6.28129283e-03, -5.11125217e-02,\n",
       "          -9.73839257e-03, -4.73865188e-02,  5.68254403e-03,\n",
       "           7.95655824e-03, -3.27830936e-03,  1.26522286e-02,\n",
       "          -2.26559777e-02,  4.02161955e-02,  3.88987813e-02,\n",
       "           1.44320133e-03, -1.08503661e-02, -3.98012355e-02,\n",
       "          -9.13996548e-02, -3.19788219e-03,  3.02327829e-02,\n",
       "          -6.07309278e-04, -1.18185127e-02, -1.70543444e-02,\n",
       "           2.37803832e-03,  6.14373823e-03,  9.88746846e-03,\n",
       "           8.45137941e-02,  2.99498128e-02,  2.23963007e-02,\n",
       "          -5.47669662e-03],\n",
       "         [ 2.26627947e-02,  4.93733792e-03, -2.42836492e-02,\n",
       "           2.17032362e-02, -1.03378051e-02,  1.55884033e-02,\n",
       "          -2.09955436e-02,  9.44756230e-03, -1.14876690e-02,\n",
       "          -1.07164164e-02, -1.61777233e-02, -3.54803664e-03,\n",
       "          -3.24473061e-02,  1.48840390e-03,  4.06399148e-03,\n",
       "           1.01711229e-02, -1.10137447e-02,  2.82472176e-02,\n",
       "          -4.37763016e-03, -1.42193200e-02,  1.31633281e-01,\n",
       "           1.22459480e-01, -5.95397199e-03,  3.07975637e-02,\n",
       "          -3.03140829e-02, -6.69818574e-03,  6.59116212e-02,\n",
       "          -2.44916352e-02, -1.19668377e-02,  2.25056209e-03,\n",
       "          -1.65040009e-02,  2.15875013e-02, -8.87567672e-03,\n",
       "           3.35288074e-02, -7.16098836e-03,  1.52244374e-03,\n",
       "           4.21342266e-02,  3.99064458e-03,  9.70893756e-03,\n",
       "           4.94446395e-04, -2.39622443e-02,  2.36851986e-02,\n",
       "           1.60506131e-02, -4.58153005e-03,  1.16669746e-02,\n",
       "           1.54997990e-02, -1.99521506e-02,  2.47698333e-02,\n",
       "           1.09411415e-02, -8.45642194e-03,  1.16298661e-02,\n",
       "           2.08235866e-02,  4.49871218e-02,  4.94169952e-03,\n",
       "          -1.00479388e-01,  1.27324367e-02,  3.42392786e-02,\n",
       "          -1.15130902e-03, -5.42923349e-03, -3.99044023e-02,\n",
       "          -1.36758559e-04,  1.96851788e-02, -2.91460424e-03,\n",
       "          -6.17680102e-04,  2.91402532e-02,  5.51162298e-02,\n",
       "          -3.47899010e-03, -2.07494115e-02, -4.91031786e-02,\n",
       "          -4.29105196e-02, -2.10036502e-03,  2.63631035e-02,\n",
       "          -3.62894784e-02,  9.97973942e-03,  4.99638948e-02,\n",
       "           2.20553371e-02, -9.79678677e-03,  1.41215943e-02,\n",
       "           2.59518344e-02, -1.27304088e-02,  1.58254954e-02,\n",
       "          -6.03992008e-03, -4.82053917e-03, -8.80735272e-04,\n",
       "          -1.59690550e-02, -4.60395145e-03,  3.52542050e-02,\n",
       "          -1.46851084e-02,  4.32766189e-03,  1.89804400e-02,\n",
       "           1.21865462e-02, -2.38298652e-02, -5.59054865e-04,\n",
       "          -3.60912434e-03, -1.23674688e-03,  6.49267027e-03,\n",
       "           1.21576086e-02, -3.19205580e-02,  7.99592342e-03,\n",
       "          -3.90810897e-02],\n",
       "         [ 1.27437921e-03, -6.68853593e-03,  2.21109384e-02,\n",
       "          -2.81375896e-02, -1.99297882e-02,  1.50635158e-02,\n",
       "           3.55956739e-02,  9.21014802e-03,  4.50974918e-02,\n",
       "          -3.64999864e-02,  2.16435831e-03, -9.44268316e-03,\n",
       "          -4.97298164e-02,  1.24322265e-02, -4.82636163e-03,\n",
       "           1.08776645e-02,  1.03911247e-02,  6.94138182e-04,\n",
       "           3.23580546e-02, -1.63804909e-02, -1.00504014e-03,\n",
       "          -1.57416420e-03,  3.81319596e-03,  2.17370080e-02,\n",
       "           3.57252308e-03,  2.44861073e-02, -2.71252028e-02,\n",
       "          -6.79720170e-04,  3.18693567e-02,  2.15974056e-02,\n",
       "           2.25400264e-03,  1.55179656e-02, -8.28223888e-03,\n",
       "           4.40871657e-02, -2.05964866e-02,  1.51543709e-02,\n",
       "          -4.86424661e-03,  5.37163608e-03, -1.41032806e-02,\n",
       "           1.51994979e-02, -3.21466495e-02,  4.85574315e-02,\n",
       "           1.90578803e-03, -9.86244844e-05,  6.70122658e-03,\n",
       "           2.57021347e-02, -3.80911923e-03,  6.97104705e-02,\n",
       "          -1.28276931e-02,  1.11882267e-02, -9.85268023e-03,\n",
       "           5.72783882e-03, -5.67307829e-02,  1.50534699e-02,\n",
       "          -8.24876942e-03, -2.43910516e-03, -3.32590986e-02,\n",
       "           5.51427694e-03, -1.71644171e-03,  3.24215713e-03,\n",
       "           8.91996461e-03, -2.66990894e-03,  6.28723635e-03,\n",
       "          -2.19682251e-02, -2.26798550e-02, -7.05978874e-03,\n",
       "           6.73034034e-03,  5.45631943e-03, -2.36107607e-03,\n",
       "           2.88332783e-02, -1.38253724e-03,  5.00538922e-04,\n",
       "          -1.54602587e-02, -1.09005592e-02,  1.91414204e-02,\n",
       "          -1.79654889e-02,  2.66945737e-02,  1.23403418e-02,\n",
       "           4.99289310e-03,  1.76388888e-03, -9.83147918e-03,\n",
       "           1.32142520e-02, -2.33681923e-02,  1.11001079e-03,\n",
       "          -2.38879386e-02, -6.36549641e-04, -2.51255661e-02,\n",
       "           1.78966815e-02, -1.76227850e-02,  7.11084189e-04,\n",
       "          -2.96103201e-02, -6.95106837e-03,  3.83876427e-03,\n",
       "           3.59189736e-03,  1.57362148e-02,  1.32255613e-02,\n",
       "          -1.62757318e-02, -1.52560845e-02, -2.60341249e-02,\n",
       "          -1.83427190e-02],\n",
       "         [-1.58931437e-02,  4.13558606e-03,  2.14644563e-02,\n",
       "           1.69600499e-02,  1.69579031e-02,  7.23437847e-02,\n",
       "          -4.65993939e-02, -1.09201681e-02, -5.20826686e-02,\n",
       "           4.17901845e-03,  1.87798106e-02, -3.59875255e-03,\n",
       "          -8.68611314e-03, -1.42885973e-02,  4.02323366e-02,\n",
       "          -1.10639893e-02,  3.39052458e-03, -3.46295822e-02,\n",
       "          -2.11280183e-02,  5.17301506e-03, -8.28297193e-02,\n",
       "          -6.72438691e-02, -3.26840877e-02,  7.85878270e-03,\n",
       "           4.40280968e-03, -1.46798674e-02,  9.05259371e-04,\n",
       "           7.48185787e-03,  1.30356550e-02, -5.80846513e-02,\n",
       "           1.28847362e-02, -1.19108840e-02,  8.43283635e-03,\n",
       "          -5.28421890e-03,  2.53309690e-03,  3.26706325e-03,\n",
       "           1.63488423e-02,  1.39643848e-03, -8.43339391e-04,\n",
       "           3.68452846e-02,  1.88963629e-02, -4.79254170e-02,\n",
       "          -9.00343075e-02, -5.67790398e-03,  3.48488502e-03,\n",
       "           1.94086927e-03, -1.24346835e-02,  1.20775843e-02,\n",
       "           1.86453104e-02,  1.86093678e-02,  1.69173304e-02,\n",
       "          -1.55660536e-02,  1.31165023e-01,  7.04569208e-03,\n",
       "           5.73362900e-02,  2.15069261e-02, -1.83936375e-02,\n",
       "          -1.14725803e-03,  8.40986361e-03,  9.21655801e-03,\n",
       "           7.75067056e-03, -1.41637374e-02, -3.08942361e-03,\n",
       "           2.24909635e-02, -1.23715535e-02, -4.35259053e-02,\n",
       "          -5.42663572e-03,  1.33733119e-02, -2.64288540e-02,\n",
       "           3.30951267e-02, -1.00225271e-02, -2.59176628e-02,\n",
       "           3.36948294e-02, -1.37528205e-02, -6.74124846e-02,\n",
       "           3.79540680e-03,  3.13441367e-02,  1.22785237e-02,\n",
       "          -2.46258570e-02,  3.71040287e-03,  2.17545637e-03,\n",
       "          -1.02975601e-02,  2.35310278e-02,  9.46252299e-03,\n",
       "          -1.81415536e-02,  1.43503098e-02, -1.62081027e-02,\n",
       "           4.89942262e-02, -1.02449915e-02, -1.71921336e-02,\n",
       "          -6.10763790e-04,  6.17695882e-03, -1.77644945e-02,\n",
       "          -1.91913773e-03,  1.82428972e-02,  3.56871891e-04,\n",
       "          -9.33743577e-02,  3.40386479e-02,  1.92469125e-02,\n",
       "           4.62915443e-02]])],\n",
       " [array([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       "  array([[ 1.15272608],\n",
       "         [ 1.51113122],\n",
       "         [ 0.66641596],\n",
       "         [ 1.03835278],\n",
       "         [ 1.43320315],\n",
       "         [-0.0168532 ],\n",
       "         [ 0.75890405],\n",
       "         [ 0.77465193],\n",
       "         [ 0.05185439],\n",
       "         [ 1.31662915],\n",
       "         [ 0.65624317],\n",
       "         [ 1.33014112],\n",
       "         [ 0.71438132],\n",
       "         [ 0.04230654],\n",
       "         [ 0.38465561],\n",
       "         [ 0.2753072 ],\n",
       "         [-0.98317271],\n",
       "         [ 1.05771308],\n",
       "         [ 0.40080391],\n",
       "         [ 1.00641381],\n",
       "         [ 1.30440683],\n",
       "         [ 0.67662562],\n",
       "         [ 0.79186241],\n",
       "         [ 0.28740786],\n",
       "         [ 1.02574833],\n",
       "         [ 0.63276021],\n",
       "         [ 0.47094999],\n",
       "         [ 1.07319905],\n",
       "         [ 0.41931875],\n",
       "         [-0.15626314],\n",
       "         [ 0.82693759],\n",
       "         [ 0.56098839],\n",
       "         [-0.83762594],\n",
       "         [ 0.77350847],\n",
       "         [-0.41516761],\n",
       "         [ 1.38199383],\n",
       "         [ 0.17093298],\n",
       "         [ 0.8215132 ],\n",
       "         [ 0.03646153],\n",
       "         [ 0.93261247],\n",
       "         [ 0.67887594],\n",
       "         [ 0.14074787],\n",
       "         [ 0.86374724],\n",
       "         [ 1.04080001],\n",
       "         [ 1.37409421],\n",
       "         [ 0.30790623],\n",
       "         [ 0.88072277],\n",
       "         [-0.05664817],\n",
       "         [ 0.74331439],\n",
       "         [ 0.42386445],\n",
       "         [ 0.50979368],\n",
       "         [ 0.34112409],\n",
       "         [ 1.12531098],\n",
       "         [-0.03187597],\n",
       "         [ 0.13294808],\n",
       "         [ 0.61958328],\n",
       "         [ 0.25200991],\n",
       "         [ 0.48729366],\n",
       "         [ 0.02672907],\n",
       "         [-0.13655587],\n",
       "         [ 0.55425425],\n",
       "         [ 0.28277565],\n",
       "         [ 0.50842663],\n",
       "         [ 0.84591133],\n",
       "         [ 0.49465901],\n",
       "         [ 1.24660627],\n",
       "         [ 0.68246045],\n",
       "         [ 0.53361208],\n",
       "         [ 0.57844919],\n",
       "         [ 0.55444361],\n",
       "         [ 0.85271109],\n",
       "         [ 1.23043004],\n",
       "         [ 0.70316396],\n",
       "         [-0.25687735],\n",
       "         [-0.03980109],\n",
       "         [ 0.31254414],\n",
       "         [ 0.2828168 ],\n",
       "         [ 0.11705801],\n",
       "         [-0.10887995],\n",
       "         [ 0.53766269],\n",
       "         [ 0.20049183],\n",
       "         [ 0.43613412],\n",
       "         [ 0.33747407],\n",
       "         [ 0.29171864],\n",
       "         [ 0.57453394],\n",
       "         [ 0.57134652],\n",
       "         [ 1.33272277],\n",
       "         [ 0.489389  ],\n",
       "         [ 1.21484682],\n",
       "         [ 0.66844284],\n",
       "         [ 1.20906804],\n",
       "         [ 1.02550628],\n",
       "         [ 1.07137401],\n",
       "         [ 1.38991877],\n",
       "         [ 0.83117291],\n",
       "         [-0.14688288],\n",
       "         [ 0.84733068],\n",
       "         [ 0.88781763],\n",
       "         [ 0.29617384],\n",
       "         [ 0.964707  ]]),\n",
       "  array([[ 0.18978506],\n",
       "         [ 1.43210143],\n",
       "         [ 0.10773208],\n",
       "         [-0.05493427],\n",
       "         [ 0.31104618],\n",
       "         [ 0.49915624],\n",
       "         [ 0.10187312],\n",
       "         [ 0.36749107],\n",
       "         [-0.24171548],\n",
       "         [-0.06127429]])])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Network(1, 100)\n",
    "net1.set_train((train_X, train_Y), 10) \n",
    "test_data = (test_X, test_y)\n",
    "test_data_tuple= [(test_X[i], test_y[i]) for i in range(len(test_data[0]))]\n",
    "net1.train(test=test_data_tuple, epochs=20, eta=0.1, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net1.evaluate(test_data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
